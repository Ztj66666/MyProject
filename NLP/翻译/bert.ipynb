{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4ff11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import dltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ce5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_segments(token_a,token_b=None):\n",
    "    tokens = ['<cls>'] + token_a + ['<sep>'] \n",
    "    segments = [0] * (len(token_a) +2)\n",
    "    if token_b is not None:\n",
    "        tokens += token_b + ['<sep>'] \n",
    "        segments += [1] * (len(token_b) +1)\n",
    "    return tokens,segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504932c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<cls>', 1, 2, 3, '<sep>', 4, 5, 6, '<sep>'], [0, 0, 0, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens_and_segments([1,2,3],[4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb64192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,num_layers,dropout,max_len=1000,key_size = 768,query_size = 768,value_size = 768, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2,num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\",dltools.EncoderBlock(key_size,query_size,value_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,dropout))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,max_len,num_hiddens))\n",
    "    def forward(self,tokens,segments,valid_lens):\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X += self.pos_embedding.data[:,:X.shape[1],:]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X,valid_lens)\n",
    "\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc9442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    def __init__(self, vocab_size,num_hiddens,num_inputs = 768, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs,num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.Linear(num_hiddens,vocab_size))\n",
    "    def forward(self,X,pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        predpositions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape(0)\n",
    "        batch_idx = torch.arange(0,batch_size)\n",
    "        bat_idx = torch.repeat_interleave(batch_idx,num_pred_positions)\n",
    "        masked_X = X[batch_idx,pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size,num_pred_positions,-1))\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f1a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        super().__init__( **kwargs)\n",
    "        self.output = nn.Linear(num_inputs,2)\n",
    "    def forward(self,X):\n",
    "        return self.output(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214b5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModule(nn.Module):\n",
    "    def __init__(self, vocab_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,num_layers,dropout,\n",
    "                 \n",
    "                 max_len=1000,key_size = 768,query_size = 768,value_size = 768,\n",
    "                 hid_in_features = 768,mlml_in_features = 768,nsp_in_features = 768,**kwargs):\n",
    "        super().__init__( **kwargs)\n",
    "        self.encoder = BERTEncoder(vocab_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,num_layers,dropout,max_len=max_len,key_size=key_size,query_size=query_size,value_size=value_size)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features,num_hiddens),nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size,num_hiddens,mlml_in_features)\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "\n",
    "    def forward(self,tokens,segments,valid_lens=None,pred_positions=None):\n",
    "        encode_X = self.encoder(tokens,segments,valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encode_X,pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encode_X[:,0,:]))\n",
    "        return encode_X,mlm_Y_hat,nsp_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd043d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
